"""
ç­–ç•¥å›æµ‹å‚æ•°ä¼˜åŒ–å™¨

æä¾›å¤šç§ä¼˜åŒ–ç®—æ³•æ¥å¯»æ‰¾ç­–ç•¥çš„æœ€ä¼˜å‚æ•°ç»„åˆï¼Œ
åŒ…æ‹¬ç½‘æ ¼æœç´¢ã€é—ä¼ ç®—æ³•ã€è´å¶æ–¯ä¼˜åŒ–ç­‰ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ç½‘æ ¼æœç´¢ä¼˜åŒ–
2. éšæœºæœç´¢ä¼˜åŒ–
3. é—ä¼ ç®—æ³•ä¼˜åŒ–
4. å‚æ•°ç©ºé—´å®šä¹‰
5. ä¼˜åŒ–ç»“æœåˆ†æ
"""

import itertools
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class ParameterRange:
    """å‚æ•°èŒƒå›´å®šä¹‰"""
    name: str                           # å‚æ•°åç§°
    param_type: str = "float"          # å‚æ•°ç±»å‹: float, int, choice
    min_value: Optional[float] = None   # æœ€å°å€¼
    max_value: Optional[float] = None   # æœ€å¤§å€¼
    step: Optional[float] = None        # æ­¥é•¿
    choices: Optional[List[Any]] = None # é€‰æ‹©åˆ—è¡¨
    
    def validate(self) -> bool:
        """éªŒè¯å‚æ•°èŒƒå›´å®šä¹‰"""
        if self.param_type in ["float", "int"]:
            return (self.min_value is not None and 
                   self.max_value is not None and
                   self.min_value < self.max_value)
        elif self.param_type == "choice":
            return self.choices is not None and len(self.choices) > 0
        return False
    
    def generate_values(self, num_samples: int = None) -> List[Any]:
        """ç”Ÿæˆå‚æ•°å€¼åˆ—è¡¨"""
        if self.param_type == "choice":
            return self.choices
        
        elif self.param_type == "int":
            if self.step is None:
                self.step = 1
            values = []
            current = self.min_value
            while current <= self.max_value:
                values.append(int(current))
                current += self.step
            return values
        
        elif self.param_type == "float":
            if num_samples is None:
                if self.step is None:
                    num_samples = 10
                else:
                    num_samples = int((self.max_value - self.min_value) / self.step) + 1
            
            if self.step is not None:
                values = []
                current = self.min_value
                while current <= self.max_value:
                    values.append(round(current, 6))
                    current += self.step
                return values
            else:
                # å‡åŒ€åˆ†å¸ƒ
                step = (self.max_value - self.min_value) / (num_samples - 1)
                return [round(self.min_value + i * step, 6) for i in range(num_samples)]
        
        return []


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    parameters: Dict[str, Any]          # å‚æ•°ç»„åˆ
    fitness: float                      # é€‚åº”åº¦å¾—åˆ†
    metrics: Dict[str, float] = field(default_factory=dict)  # è¯¦ç»†æŒ‡æ ‡
    backtest_results: Any = None        # å›æµ‹ç»“æœå¯¹è±¡
    optimization_time: float = 0.0      # ä¼˜åŒ–è€—æ—¶
    
    def __lt__(self, other):
        """ç”¨äºæ’åºæ¯”è¾ƒ"""
        return self.fitness < other.fitness


class ParameterOptimizer(ABC):
    """å‚æ•°ä¼˜åŒ–å™¨åŸºç±»"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"Optimizer.{name}")
    
    @abstractmethod
    def optimize(self, parameter_ranges: List[ParameterRange],
                objective_function: Callable,
                max_iterations: int = 100) -> List[OptimizationResult]:
        """æ‰§è¡Œå‚æ•°ä¼˜åŒ–"""
        pass


class GridSearchOptimizer(ParameterOptimizer):
    """ç½‘æ ¼æœç´¢ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        super().__init__("GridSearch")
    
    def optimize(self, parameter_ranges: List[ParameterRange],
                objective_function: Callable,
                max_iterations: int = 100) -> List[OptimizationResult]:
        """
        ç½‘æ ¼æœç´¢ä¼˜åŒ–
        
        Args:
            parameter_ranges: å‚æ•°èŒƒå›´åˆ—è¡¨
            objective_function: ç›®æ ‡å‡½æ•°ï¼Œæ¥å—å‚æ•°å­—å…¸ï¼Œè¿”å›OptimizationResult
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Returns:
            ä¼˜åŒ–ç»“æœåˆ—è¡¨ï¼ŒæŒ‰é€‚åº”åº¦æ’åº
        """
        self.logger.info("å¼€å§‹ç½‘æ ¼æœç´¢ä¼˜åŒ–")
        
        # éªŒè¯å‚æ•°èŒƒå›´
        for param_range in parameter_ranges:
            if not param_range.validate():
                raise ValueError(f"æ— æ•ˆçš„å‚æ•°èŒƒå›´: {param_range.name}")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_combinations = self._generate_grid_combinations(parameter_ranges)
        
        # é™åˆ¶ç»„åˆæ•°é‡
        if len(param_combinations) > max_iterations:
            self.logger.warning(f"å‚æ•°ç»„åˆè¿‡å¤š({len(param_combinations)})ï¼Œé™åˆ¶ä¸º{max_iterations}")
            # ä½¿ç”¨é‡‡æ ·å‡å°‘ç»„åˆæ•°é‡
            import random
            random.seed(42)
            param_combinations = random.sample(param_combinations, max_iterations)
        
        results = []
        total_combinations = len(param_combinations)
        
        self.logger.info(f"æ€»å‚æ•°ç»„åˆæ•°: {total_combinations}")
        
        # æ‰§è¡Œå›æµ‹
        for i, params in enumerate(param_combinations):
            try:
                start_time = datetime.now()
                
                # è°ƒç”¨ç›®æ ‡å‡½æ•°
                result = objective_function(params)
                
                if isinstance(result, OptimizationResult):
                    result.optimization_time = (datetime.now() - start_time).total_seconds()
                    results.append(result)
                else:
                    # å…¼å®¹è¿”å›å•ä¸ªæ•°å€¼çš„æƒ…å†µ
                    opt_result = OptimizationResult(
                        parameters=params,
                        fitness=float(result),
                        optimization_time=(datetime.now() - start_time).total_seconds()
                    )
                    results.append(opt_result)
                
                # è¿›åº¦æ—¥å¿—
                if (i + 1) % max(1, total_combinations // 10) == 0:
                    progress = (i + 1) / total_combinations * 100
                    self.logger.info(f"ä¼˜åŒ–è¿›åº¦: {progress:.1f}% ({i+1}/{total_combinations})")
            
            except Exception as e:
                self.logger.error(f"å‚æ•°ç»„åˆ{i+1}ä¼˜åŒ–å¤±è´¥: {params}, {e}")
                continue
        
        # æŒ‰é€‚åº”åº¦æ’åºï¼ˆé™åºï¼‰
        results.sort(key=lambda x: x.fitness, reverse=True)
        
        self.logger.info(f"ç½‘æ ¼æœç´¢å®Œæˆ: {len(results)}ä¸ªæœ‰æ•ˆç»“æœ")
        if results:
            best = results[0]
            self.logger.info(f"æœ€ä¼˜ç»“æœ: é€‚åº”åº¦={best.fitness:.4f}, å‚æ•°={best.parameters}")
        
        return results
    
    def _generate_grid_combinations(self, parameter_ranges: List[ParameterRange]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç½‘æ ¼å‚æ•°ç»„åˆ"""
        param_values = {}
        
        for param_range in parameter_ranges:
            param_values[param_range.name] = param_range.generate_values()
        
        # ç”Ÿæˆç¬›å¡å°”ç§¯
        param_names = list(param_values.keys())
        value_combinations = itertools.product(*[param_values[name] for name in param_names])
        
        combinations = []
        for values in value_combinations:
            combination = dict(zip(param_names, values))
            combinations.append(combination)
        
        return combinations


class RandomSearchOptimizer(ParameterOptimizer):
    """éšæœºæœç´¢ä¼˜åŒ–å™¨"""
    
    def __init__(self, seed: int = 42):
        super().__init__("RandomSearch")
        import random
        self.random = random
        self.random.seed(seed)
    
    def optimize(self, parameter_ranges: List[ParameterRange],
                objective_function: Callable,
                max_iterations: int = 100) -> List[OptimizationResult]:
        """
        éšæœºæœç´¢ä¼˜åŒ–
        
        åœ¨å‚æ•°ç©ºé—´ä¸­éšæœºé‡‡æ ·ï¼Œé€‚åˆé«˜ç»´å‚æ•°ç©ºé—´ã€‚
        """
        self.logger.info("å¼€å§‹éšæœºæœç´¢ä¼˜åŒ–")
        
        # éªŒè¯å‚æ•°èŒƒå›´
        for param_range in parameter_ranges:
            if not param_range.validate():
                raise ValueError(f"æ— æ•ˆçš„å‚æ•°èŒƒå›´: {param_range.name}")
        
        results = []
        
        for i in range(max_iterations):
            try:
                # éšæœºç”Ÿæˆå‚æ•°ç»„åˆ
                params = self._generate_random_parameters(parameter_ranges)
                
                start_time = datetime.now()
                
                # è°ƒç”¨ç›®æ ‡å‡½æ•°
                result = objective_function(params)
                
                if isinstance(result, OptimizationResult):
                    result.optimization_time = (datetime.now() - start_time).total_seconds()
                    results.append(result)
                else:
                    opt_result = OptimizationResult(
                        parameters=params,
                        fitness=float(result),
                        optimization_time=(datetime.now() - start_time).total_seconds()
                    )
                    results.append(opt_result)
                
                # è¿›åº¦æ—¥å¿—
                if (i + 1) % max(1, max_iterations // 10) == 0:
                    progress = (i + 1) / max_iterations * 100
                    self.logger.info(f"ä¼˜åŒ–è¿›åº¦: {progress:.1f}% ({i+1}/{max_iterations})")
            
            except Exception as e:
                self.logger.error(f"éšæœºæœç´¢ç¬¬{i+1}æ¬¡å¤±è´¥: {e}")
                continue
        
        # æ’åºç»“æœ
        results.sort(key=lambda x: x.fitness, reverse=True)
        
        self.logger.info(f"éšæœºæœç´¢å®Œæˆ: {len(results)}ä¸ªæœ‰æ•ˆç»“æœ")
        if results:
            best = results[0]
            self.logger.info(f"æœ€ä¼˜ç»“æœ: é€‚åº”åº¦={best.fitness:.4f}, å‚æ•°={best.parameters}")
        
        return results
    
    def _generate_random_parameters(self, parameter_ranges: List[ParameterRange]) -> Dict[str, Any]:
        """ç”Ÿæˆéšæœºå‚æ•°ç»„åˆ"""
        params = {}
        
        for param_range in parameter_ranges:
            if param_range.param_type == "choice":
                params[param_range.name] = self.random.choice(param_range.choices)
            
            elif param_range.param_type == "int":
                params[param_range.name] = self.random.randint(
                    int(param_range.min_value), 
                    int(param_range.max_value)
                )
            
            elif param_range.param_type == "float":
                params[param_range.name] = round(
                    self.random.uniform(param_range.min_value, param_range.max_value), 
                    6
                )
        
        return params


class GeneticOptimizer(ParameterOptimizer):
    """é—ä¼ ç®—æ³•ä¼˜åŒ–å™¨"""
    
    def __init__(self, population_size: int = 50, mutation_rate: float = 0.1, 
                 crossover_rate: float = 0.8, seed: int = 42):
        super().__init__("Genetic")
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        
        import random
        self.random = random
        self.random.seed(seed)
    
    def optimize(self, parameter_ranges: List[ParameterRange],
                objective_function: Callable,
                max_iterations: int = 100) -> List[OptimizationResult]:
        """
        é—ä¼ ç®—æ³•ä¼˜åŒ–
        
        ä½¿ç”¨é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚æ“ä½œè¿›åŒ–å‚æ•°ç»„åˆã€‚
        """
        self.logger.info(f"å¼€å§‹é—ä¼ ç®—æ³•ä¼˜åŒ–: ç§ç¾¤å¤§å°={self.population_size}, ä»£æ•°={max_iterations}")
        
        # éªŒè¯å‚æ•°èŒƒå›´
        for param_range in parameter_ranges:
            if not param_range.validate():
                raise ValueError(f"æ— æ•ˆçš„å‚æ•°èŒƒå›´: {param_range.name}")
        
        self.parameter_ranges = parameter_ranges
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = self._initialize_population()
        all_results = []
        
        for generation in range(max_iterations):
            self.logger.info(f"ç¬¬{generation+1}ä»£è¿›åŒ–")
            
            # è¯„ä¼°ç§ç¾¤
            generation_results = []
            for individual in population:
                try:
                    start_time = datetime.now()
                    result = objective_function(individual)
                    
                    if isinstance(result, OptimizationResult):
                        result.optimization_time = (datetime.now() - start_time).total_seconds()
                        generation_results.append(result)
                    else:
                        opt_result = OptimizationResult(
                            parameters=individual,
                            fitness=float(result),
                            optimization_time=(datetime.now() - start_time).total_seconds()
                        )
                        generation_results.append(opt_result)
                
                except Exception as e:
                    self.logger.error(f"ä¸ªä½“è¯„ä¼°å¤±è´¥: {individual}, {e}")
                    # ç»™å¤±è´¥çš„ä¸ªä½“ä¸€ä¸ªå¾ˆä½çš„é€‚åº”åº¦
                    opt_result = OptimizationResult(
                        parameters=individual,
                        fitness=-float('inf')
                    )
                    generation_results.append(opt_result)
            
            all_results.extend(generation_results)
            
            # æ’åºï¼ˆæŒ‰é€‚åº”åº¦é™åºï¼‰
            generation_results.sort(key=lambda x: x.fitness, reverse=True)
            
            # è®°å½•æœ€ä¼˜ä¸ªä½“
            if generation_results:
                best = generation_results[0]
                avg_fitness = sum(r.fitness for r in generation_results if r.fitness != -float('inf')) / len(generation_results)
                self.logger.info(f"ç¬¬{generation+1}ä»£: æœ€ä¼˜é€‚åº”åº¦={best.fitness:.4f}, å¹³å‡é€‚åº”åº¦={avg_fitness:.4f}")
            
            # é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
            if generation < max_iterations - 1:  # ä¸æ˜¯æœ€åä¸€ä»£
                population = self._evolve_population(generation_results)
        
        # è¿”å›æ‰€æœ‰ç»“æœï¼ŒæŒ‰é€‚åº”åº¦æ’åº
        all_results.sort(key=lambda x: x.fitness, reverse=True)
        
        self.logger.info(f"é—ä¼ ç®—æ³•å®Œæˆ: {len(all_results)}ä¸ªè¯„ä¼°ç»“æœ")
        if all_results:
            best = all_results[0]
            self.logger.info(f"å…¨å±€æœ€ä¼˜: é€‚åº”åº¦={best.fitness:.4f}, å‚æ•°={best.parameters}")
        
        return all_results
    
    def _initialize_population(self) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ–ç§ç¾¤"""
        population = []
        
        for _ in range(self.population_size):
            individual = {}
            for param_range in self.parameter_ranges:
                if param_range.param_type == "choice":
                    individual[param_range.name] = self.random.choice(param_range.choices)
                elif param_range.param_type == "int":
                    individual[param_range.name] = self.random.randint(
                        int(param_range.min_value), 
                        int(param_range.max_value)
                    )
                elif param_range.param_type == "float":
                    individual[param_range.name] = round(
                        self.random.uniform(param_range.min_value, param_range.max_value), 
                        6
                    )
            population.append(individual)
        
        return population
    
    def _evolve_population(self, evaluated_population: List[OptimizationResult]) -> List[Dict[str, Any]]:
        """è¿›åŒ–ç§ç¾¤"""
        # é€‰æ‹©ï¼ˆè½®ç›˜èµŒé€‰æ‹©ï¼‰
        selected = self._selection(evaluated_population)
        
        # äº¤å‰å’Œå˜å¼‚
        new_population = []
        
        for i in range(0, len(selected), 2):
            parent1 = selected[i].parameters
            parent2 = selected[i + 1] if i + 1 < len(selected) else selected[0].parameters
            
            # äº¤å‰
            if self.random.random() < self.crossover_rate:
                child1, child2 = self._crossover(parent1, parent2)
            else:
                child1, child2 = parent1.copy(), parent2.copy()
            
            # å˜å¼‚
            if self.random.random() < self.mutation_rate:
                child1 = self._mutate(child1)
            if self.random.random() < self.mutation_rate:
                child2 = self._mutate(child2)
            
            new_population.extend([child1, child2])
        
        # ç¡®ä¿ç§ç¾¤å¤§å°
        return new_population[:self.population_size]
    
    def _selection(self, evaluated_population: List[OptimizationResult]) -> List[OptimizationResult]:
        """è½®ç›˜èµŒé€‰æ‹©"""
        # å¤„ç†è´Ÿé€‚åº”åº¦
        min_fitness = min(r.fitness for r in evaluated_population if r.fitness != -float('inf'))
        if min_fitness < 0:
            offset = abs(min_fitness) + 1
        else:
            offset = 0
        
        # è®¡ç®—é€‰æ‹©æ¦‚ç‡
        total_fitness = sum(max(0, r.fitness + offset) for r in evaluated_population)
        
        if total_fitness == 0:
            # å¦‚æœæ‰€æœ‰é€‚åº”åº¦éƒ½ç›¸åŒï¼Œéšæœºé€‰æ‹©
            return self.random.choices(evaluated_population, k=self.population_size)
        
        # è½®ç›˜èµŒé€‰æ‹©
        selected = []
        for _ in range(self.population_size):
            pick = self.random.uniform(0, total_fitness)
            current = 0
            for individual in evaluated_population:
                current += max(0, individual.fitness + offset)
                if current >= pick:
                    selected.append(individual)
                    break
        
        return selected
    
    def _crossover(self, parent1: Dict[str, Any], parent2: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """å•ç‚¹äº¤å‰"""
        child1 = parent1.copy()
        child2 = parent2.copy()
        
        # éšæœºé€‰æ‹©äº¤å‰ç‚¹
        param_names = list(parent1.keys())
        if len(param_names) > 1:
            crossover_point = self.random.randint(1, len(param_names) - 1)
            
            for i, param_name in enumerate(param_names):
                if i >= crossover_point:
                    child1[param_name] = parent2[param_name]
                    child2[param_name] = parent1[param_name]
        
        return child1, child2
    
    def _mutate(self, individual: Dict[str, Any]) -> Dict[str, Any]:
        """å˜å¼‚æ“ä½œ"""
        mutated = individual.copy()
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªå‚æ•°è¿›è¡Œå˜å¼‚
        param_name = self.random.choice(list(mutated.keys()))
        
        # æ‰¾åˆ°å¯¹åº”çš„å‚æ•°èŒƒå›´
        param_range = None
        for pr in self.parameter_ranges:
            if pr.name == param_name:
                param_range = pr
                break
        
        if param_range:
            if param_range.param_type == "choice":
                mutated[param_name] = self.random.choice(param_range.choices)
            elif param_range.param_type == "int":
                mutated[param_name] = self.random.randint(
                    int(param_range.min_value), 
                    int(param_range.max_value)
                )
            elif param_range.param_type == "float":
                mutated[param_name] = round(
                    self.random.uniform(param_range.min_value, param_range.max_value), 
                    6
                )
        
        return mutated


class OptimizationManager:
    """
    å‚æ•°ä¼˜åŒ–ç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†å¤šç§ä¼˜åŒ–ç®—æ³•ï¼Œæä¾›ç®€åŒ–çš„æ¥å£
    è¿›è¡Œç­–ç•¥å‚æ•°ä¼˜åŒ–ã€‚
    """
    
    def __init__(self):
        self.optimizers = {
            "grid": GridSearchOptimizer(),
            "random": RandomSearchOptimizer(),
            "genetic": GeneticOptimizer()
        }
        self.logger = logging.getLogger("OptimizationManager")
    
    def register_optimizer(self, name: str, optimizer: ParameterOptimizer):
        """æ³¨å†Œæ–°çš„ä¼˜åŒ–å™¨"""
        self.optimizers[name] = optimizer
        self.logger.info(f"æ³¨å†Œä¼˜åŒ–å™¨: {name}")
    
    def optimize_strategy(self, 
                         parameter_ranges: List[ParameterRange],
                         backtest_function: Callable,
                         objective_metric: str = "sharpe_ratio",
                         optimizer_type: str = "grid",
                         max_iterations: int = 100) -> List[OptimizationResult]:
        """
        ç­–ç•¥å‚æ•°ä¼˜åŒ–
        
        Args:
            parameter_ranges: å‚æ•°èŒƒå›´åˆ—è¡¨
            backtest_function: å›æµ‹å‡½æ•°ï¼Œæ¥å—å‚æ•°å­—å…¸ï¼Œè¿”å›å›æµ‹ç»“æœ
            objective_metric: ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡
            optimizer_type: ä¼˜åŒ–å™¨ç±»å‹
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Returns:
            ä¼˜åŒ–ç»“æœåˆ—è¡¨
        """
        if optimizer_type not in self.optimizers:
            raise ValueError(f"æœªçŸ¥çš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")
        
        optimizer = self.optimizers[optimizer_type]
        
        def objective_function(params: Dict[str, Any]) -> OptimizationResult:
            """ç›®æ ‡å‡½æ•°"""
            try:
                # æ‰§è¡Œå›æµ‹
                backtest_result = backtest_function(params)
                
                # æå–ç›®æ ‡æŒ‡æ ‡
                if hasattr(backtest_result, objective_metric):
                    fitness = getattr(backtest_result, objective_metric)
                elif hasattr(backtest_result, 'metrics') and objective_metric in backtest_result.metrics:
                    fitness = backtest_result.metrics[objective_metric]
                else:
                    # é»˜è®¤ä½¿ç”¨æ€»æ”¶ç›Š
                    fitness = getattr(backtest_result, 'total_return', 0.0)
                
                # åˆ›å»ºä¼˜åŒ–ç»“æœ
                metrics = {}
                if hasattr(backtest_result, 'get_summary'):
                    summary = backtest_result.get_summary()
                    if 'performance' in summary:
                        for key, value in summary['performance'].items():
                            try:
                                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                                if isinstance(value, str) and '%' in value:
                                    metrics[key] = float(value.replace('%', '')) / 100
                                else:
                                    metrics[key] = float(value)
                            except:
                                pass
                
                return OptimizationResult(
                    parameters=params,
                    fitness=float(fitness) if fitness is not None else 0.0,
                    metrics=metrics,
                    backtest_results=backtest_result
                )
            
            except Exception as e:
                self.logger.error(f"å›æµ‹å¤±è´¥: {params}, {e}")
                return OptimizationResult(
                    parameters=params,
                    fitness=-float('inf')
                )
        
        # æ‰§è¡Œä¼˜åŒ–
        self.logger.info(f"å¼€å§‹å‚æ•°ä¼˜åŒ–: ç®—æ³•={optimizer_type}, ç›®æ ‡={objective_metric}")
        results = optimizer.optimize(parameter_ranges, objective_function, max_iterations)
        
        self.logger.info(f"å‚æ•°ä¼˜åŒ–å®Œæˆ: {len(results)}ä¸ªç»“æœ")
        
        return results
    
    def get_optimization_report(self, results: List[OptimizationResult], 
                               top_n: int = 10) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        if not results:
            return {"error": "æ²¡æœ‰ä¼˜åŒ–ç»“æœ"}
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = [r for r in results if r.fitness != -float('inf')]
        
        if not valid_results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„ä¼˜åŒ–ç»“æœ"}
        
        # åŸºç¡€ç»Ÿè®¡
        fitness_values = [r.fitness for r in valid_results]
        
        report = {
            "summary": {
                "total_evaluations": len(results),
                "valid_evaluations": len(valid_results),
                "success_rate": len(valid_results) / len(results),
                "best_fitness": max(fitness_values),
                "worst_fitness": min(fitness_values),
                "average_fitness": sum(fitness_values) / len(fitness_values),
                "fitness_std": self._calculate_std(fitness_values)
            },
            "top_results": [
                {
                    "rank": i + 1,
                    "parameters": result.parameters,
                    "fitness": result.fitness,
                    "metrics": result.metrics
                }
                for i, result in enumerate(valid_results[:top_n])
            ],
            "parameter_analysis": self._analyze_parameters(valid_results)
        }
        
        return report
    
    def _calculate_std(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance ** 0.5
    
    def _analyze_parameters(self, results: List[OptimizationResult]) -> Dict[str, Any]:
        """åˆ†æå‚æ•°åˆ†å¸ƒ"""
        if not results:
            return {}
        
        param_analysis = {}
        
        # è·å–æ‰€æœ‰å‚æ•°å
        param_names = set()
        for result in results:
            param_names.update(result.parameters.keys())
        
        for param_name in param_names:
            values = []
            for result in results:
                if param_name in result.parameters:
                    values.append(result.parameters[param_name])
            
            if values:
                if isinstance(values[0], (int, float)):
                    param_analysis[param_name] = {
                        "type": "numeric",
                        "min": min(values),
                        "max": max(values),
                        "mean": sum(values) / len(values),
                        "best_value": results[0].parameters.get(param_name)
                    }
                else:
                    # åˆ†ç±»å‚æ•°
                    value_counts = {}
                    for value in values:
                        value_counts[value] = value_counts.get(value, 0) + 1
                    
                    param_analysis[param_name] = {
                        "type": "categorical",
                        "value_counts": value_counts,
                        "best_value": results[0].parameters.get(param_name)
                    }
        
        return param_analysis


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸ”§ ç­–ç•¥å›æµ‹å‚æ•°ä¼˜åŒ–å™¨")
    print("=" * 50)
    
    # åˆ›å»ºä¼˜åŒ–ç®¡ç†å™¨
    optimization_manager = OptimizationManager()
    print("âœ… ä¼˜åŒ–ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
    
    # å®šä¹‰ç¤ºä¾‹å‚æ•°èŒƒå›´
    parameter_ranges = [
        ParameterRange(name="period", param_type="int", min_value=5, max_value=30, step=5),
        ParameterRange(name="threshold", param_type="float", min_value=0.01, max_value=0.1, step=0.01),
        ParameterRange(name="method", param_type="choice", choices=["sma", "ema", "wma"])
    ]
    
    print("\\nğŸ“Š å‚æ•°èŒƒå›´å®šä¹‰:")
    for pr in parameter_ranges:
        print(f"  {pr.name}: {pr.param_type}, èŒƒå›´={pr.min_value}-{pr.max_value}")
        if pr.choices:
            print(f"    é€‰æ‹©: {pr.choices}")
    
    # æµ‹è¯•ç½‘æ ¼æœç´¢
    print("\\nğŸ” æµ‹è¯•ç½‘æ ¼æœç´¢...")
    grid_optimizer = GridSearchOptimizer()
    
    # æ¨¡æ‹Ÿç›®æ ‡å‡½æ•°
    def mock_objective(params):
        # æ¨¡æ‹Ÿè®¡ç®—é€‚åº”åº¦
        fitness = params.get("period", 10) * 0.1 + params.get("threshold", 0.05) * 10
        if params.get("method") == "ema":
            fitness += 0.5
        
        return OptimizationResult(
            parameters=params,
            fitness=fitness,
            metrics={"return": fitness * 0.1, "sharpe": fitness * 0.2}
        )
    
    grid_results = grid_optimizer.optimize(parameter_ranges, mock_objective, max_iterations=50)
    print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆ: {len(grid_results)}ä¸ªç»“æœ")
    if grid_results:
        best = grid_results[0]
        print(f"  æœ€ä¼˜å‚æ•°: {best.parameters}")
        print(f"  æœ€ä¼˜é€‚åº”åº¦: {best.fitness:.4f}")
    
    # æµ‹è¯•éšæœºæœç´¢
    print("\\nğŸ² æµ‹è¯•éšæœºæœç´¢...")
    random_optimizer = RandomSearchOptimizer()
    random_results = random_optimizer.optimize(parameter_ranges, mock_objective, max_iterations=30)
    print(f"âœ… éšæœºæœç´¢å®Œæˆ: {len(random_results)}ä¸ªç»“æœ")
    if random_results:
        best = random_results[0]
        print(f"  æœ€ä¼˜å‚æ•°: {best.parameters}")
        print(f"  æœ€ä¼˜é€‚åº”åº¦: {best.fitness:.4f}")
    
    # æµ‹è¯•é—ä¼ ç®—æ³•
    print("\\nğŸ§¬ æµ‹è¯•é—ä¼ ç®—æ³•...")
    genetic_optimizer = GeneticOptimizer(population_size=20, mutation_rate=0.2)
    genetic_results = genetic_optimizer.optimize(parameter_ranges, mock_objective, max_iterations=10)
    print(f"âœ… é—ä¼ ç®—æ³•å®Œæˆ: {len(genetic_results)}ä¸ªç»“æœ")
    if genetic_results:
        best = genetic_results[0]
        print(f"  æœ€ä¼˜å‚æ•°: {best.parameters}")
        print(f"  æœ€ä¼˜é€‚åº”åº¦: {best.fitness:.4f}")
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\\nğŸ“ˆ ä¼˜åŒ–æŠ¥å‘Š...")
    if grid_results:
        report = optimization_manager.get_optimization_report(grid_results, top_n=5)
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        print(f"  æ€»è¯„ä¼°æ¬¡æ•°: {report['summary']['total_evaluations']}")
        print(f"  æˆåŠŸç‡: {report['summary']['success_rate']:.1%}")
        print(f"  æœ€ä¼˜é€‚åº”åº¦: {report['summary']['best_fitness']:.4f}")
        print(f"  å‚æ•°åˆ†æ: {len(report['parameter_analysis'])}ä¸ªå‚æ•°")
    
    print("\\nğŸ¯ å‚æ•°ä¼˜åŒ–å™¨æ ¸å¿ƒåŠŸèƒ½:")
    print("  - ç½‘æ ¼æœç´¢ç®—æ³• âœ…")
    print("  - éšæœºæœç´¢ç®—æ³• âœ…")
    print("  - é—ä¼ ç®—æ³•ä¼˜åŒ– âœ…")
    print("  - å‚æ•°èŒƒå›´ç®¡ç† âœ…")
    print("  - ä¼˜åŒ–ç»“æœåˆ†æ âœ…")
    
    print("\\nğŸ”§ ä¸‹ä¸€æ­¥é›†æˆ:")
    print("  1. è´å¶æ–¯ä¼˜åŒ–ç®—æ³•")
    print("  2. å¤šç›®æ ‡ä¼˜åŒ–æ”¯æŒ")
    print("  3. å¹¶è¡Œä¼˜åŒ–æ‰§è¡Œ")
    print("  4. ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–")
    
    print("\\n" + "=" * 50)